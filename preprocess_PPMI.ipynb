{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cf897c",
   "metadata": {},
   "source": [
    "# PPMI preprocessing\n",
    "\n",
    "This notebook can be used to convert all raw data into the desired format for baseline training and MAGNN implementation. \n",
    "\n",
    "This notebook contains the following steps:\n",
    "- [1. Use dataloader class](#use_dataloader_class)\n",
    "- [2. Convert 4 data components into MAGNN readable format](#convert_to_maggn_format)\n",
    "- [3. Create train-test split](#create_train_test_split)\n",
    "- [4. Define metapaths and obtain metapath instances](#define_metapaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2398b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.data import load_PPMI_data\n",
    "from collections import ChainMap\n",
    "\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "import utils.preprocess\n",
    "\n",
    "import data_utils as du\n",
    "from PPMI_dataloader import *\n",
    "\n",
    "data_dir = du.find_data_dir('app')\n",
    "dataloader_file = du.get_file_path(data_dir, 'class based structure', 'dataloaders', 'dataloader.p')\n",
    "metapaths_metabolites_file = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', 'PPMI_neighbor_pairs.p')\n",
    "val_train_test_idx_file = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', 'val_train_test_idx.npz')\n",
    "metabolite_matcher_file = du.get_file_path(data_dir, 'class based structure', 'metabolite matching', 'matcher.p')\n",
    "\n",
    "#MAGNN input file names\n",
    "adjM_file = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', 'adjM.npz')\n",
    "features_proteins_file = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', 'features_proteins.npz')\n",
    "features_metabolites_file = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', 'features_metabolites.npz')\n",
    "node_types_file = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', 'node_types.npy')\n",
    "labels_file = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', 'labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077a888",
   "metadata": {},
   "source": [
    "<a id='use_dataloader_class'></a>\n",
    "## 1. Use DataLoader class\n",
    "\n",
    "This notebook builds on the `DataLoader`, `ExerciseMetabolomicsDataLoader` and `MetaboliteNameMatcher`  classes in `PPMI_dataloader.py`\n",
    "\n",
    "#### Naive CCS method\n",
    "The default implementation uses the naive method to obtain concentration change sign (CCS). With $\\Delta$ as mean log2fold change across subjects per metabolite\n",
    "\n",
    "   $\\text{CCS}(\\Delta) =\n",
    "    \\left\\{ \\begin{matrix}\n",
    "     1 & p < \\Delta \\\\ \n",
    "     0 & -p \\leq \\Delta \\leq p \\\\ \n",
    "     -1 & \\Delta < -p \n",
    "    \\end{matrix}\n",
    "    \\right. $\n",
    "\n",
    "#### Network pruning\n",
    "The network pruning is implemented as visualized in the figure.\n",
    "<div> <img src=\"images/pruning_the_network.png\" width=\"800\"/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d6079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of metabolites in publication Millan 2020: 368\n",
      "Number of unnamed metabolites in publication Millan 2020: 14\n",
      "Number of metabolites in publication Millan 2020 after removing unnamed: 354\n",
      "Number of metabolites with KEGG id, without HMDB id: 29\n",
      "Number of metabolites with HMDB id and data: 325\n",
      "Number of unique duplicated HMDB ids: 9\n",
      "Number of removed rows becuase target value was not allowed: 0\n",
      "Total number of metabolites in full PPMI: 20759\n",
      "Total number of metabolites with CCS and not in PPMI: 82\n",
      "Number of metabolites with CCS data in full PPMI: 261\n",
      "\n",
      "Number of valid metabolites with CCS data in full PPMI (y): 228\n",
      "Number of metabolite feature columns (X): 253\n"
     ]
    }
   ],
   "source": [
    "calc_CCS_settings = {'method': 'naive', \n",
    "                     'p_value': 0.1}\n",
    "\n",
    "allowed_ccs_values = [-1, 0, 1]\n",
    "\n",
    "include_feature_protein = {'molecular_function':       True,\n",
    "                           'biological_process':       True,\n",
    "                           'protein_class':            True}\n",
    "\n",
    "include_feature_metabolite = {'molecular_weight':      False,\n",
    "                              'state':                 True,\n",
    "                              'kingdom':               True,\n",
    "                              'super_class':           True,\n",
    "                              'class':                 False,\n",
    "                              'direct_parent':         False,\n",
    "                              'molecular_framework':   True,\n",
    "                              'alternative_parents':   True,\n",
    "                              'substituents':          True,\n",
    "                              'external_descriptors':  False,\n",
    "                              'cellular_locations':    True,\n",
    "                              'biospecimen_locations': True,\n",
    "                              'tissue_locations':      True,\n",
    "                              'pathways':              False}\n",
    "\n",
    "include_feature_category = {'metabolite': include_feature_metabolite,\n",
    "                            'protein': include_feature_protein}\n",
    "\n",
    "publication_names = ['millan']\n",
    "\n",
    "dataloader = DataLoader(publication_names, include_feature_category, metabolite_matcher_file=metabolite_matcher_file, calc_CCS_settings=calc_CCS_settings, allowed_ccs_values=allowed_ccs_values)\n",
    "dataloader.save_state()\n",
    "dataloader.print_data_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc11cf2",
   "metadata": {},
   "source": [
    "### Save MetaboliteMatcher object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1277906",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = dataloader.metabolite_matcher\n",
    "du.dump_in_pickle(metabolite_matcher_file, matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c4095",
   "metadata": {},
   "source": [
    "### Load saved Dataloader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a850b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = du.read_from_pickle(dataloader_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d470b",
   "metadata": {},
   "source": [
    "<a id='convert_to_maggn_format'></a>\n",
    "## MAGNN preprocessing: convert data from dataloader into MAGNN readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69cbd89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CCS_class(CCS, CCS_classes):\n",
    "    return CCS_classes[CCS]\n",
    "\n",
    "CCS_classes = {-1: 'down',\n",
    "                0: 'unchanged',\n",
    "                1: 'up'}\n",
    "\n",
    "CCS_labels_binary = {'down': 0,\n",
    "                     'up': 1}\n",
    "\n",
    "CCS_labels_multi_class = {'down': 0,\n",
    "                          'unchanged': 1,\n",
    "                          'up': 2}\n",
    "\n",
    "types = {'metabolite': 0,\n",
    "         'protein': 1}\n",
    "\n",
    "PPMI_adjM = nx.convert_matrix.to_numpy_array(dataloader.PPMI_pruned)\n",
    "PPMI_type_mask = np.array([types['metabolite'] if 'HMDB' in node else types['protein'] for node in dataloader.PPMI_pruned.nodes()], dtype='int32')\n",
    "\n",
    "#0 for down and 1 for up (binary case)\n",
    "#0 for down and 1 for unchanged and 2 for up (multi class case)\n",
    "classes = dataloader.y.apply(get_CCS_class, args=(CCS_classes,))\n",
    "multi_class_labels = classes.apply(get_CCS_class, args=(CCS_labels_multi_class,))\n",
    "PPMI_labels = multi_class_labels\n",
    "\n",
    "scipy.sparse.save_npz(adjM_file, scipy.sparse.csr_matrix(PPMI_adjM))\n",
    "scipy.sparse.save_npz(features_metabolites_file, scipy.sparse.csr_matrix(dataloader.X))\n",
    "scipy.sparse.save_npz(features_proteins_file, scipy.sparse.csr_matrix(dataloader.protein_features))\n",
    "np.save(labels_file, PPMI_labels)\n",
    "np.save(node_types_file, PPMI_type_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e9be369",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proteins = pd.Series(PPMI_type_mask).value_counts()[types['protein']]\n",
    "n_metabolites = pd.Series(PPMI_type_mask).value_counts()[types['metabolite']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e89e1",
   "metadata": {},
   "source": [
    "<a id='create_train_test_split'></a>\n",
    "## Creating train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "08bbe7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_d46f1_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Size</th>        <th class=\"col_heading level0 col1\" >%</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_d46f1_level0_row0\" class=\"row_heading level0 row0\" >Train</th>\n",
       "                        <td id=\"T_d46f1_row0_col0\" class=\"data row0 col0\" >81</td>\n",
       "                        <td id=\"T_d46f1_row0_col1\" class=\"data row0 col1\" >35.5%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d46f1_level0_row1\" class=\"row_heading level0 row1\" >Val</th>\n",
       "                        <td id=\"T_d46f1_row1_col0\" class=\"data row1 col0\" >55</td>\n",
       "                        <td id=\"T_d46f1_row1_col1\" class=\"data row1 col1\" >24.1%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d46f1_level0_row2\" class=\"row_heading level0 row2\" >Test</th>\n",
       "                        <td id=\"T_d46f1_row2_col0\" class=\"data row2 col0\" >92</td>\n",
       "                        <td id=\"T_d46f1_row2_col1\" class=\"data row2 col1\" >40.4%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6bdd0e99a0>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_val_test_stats(PPMI_labels, train_idx, val_idx, test_idx):\n",
    "    n_samples = len(PPMI_labels)\n",
    "    df = pd.DataFrame([['Train', len(train_idx), len(train_idx)/n_samples*100], \n",
    "    ['Val', len(val_idx), len(val_idx)/n_samples*100],\n",
    "    ['Test', len(test_idx), len(test_idx)/n_samples*100]], columns = ['Group', 'Size', '%']).set_index('Group').style.format({'Size': \"{:.0f}\", \"%\": \"{:.1f}%\"})\n",
    "    df.index.name = None\n",
    "    return df  \n",
    "\n",
    "rand_seed = 15669114\n",
    "train_idx, test_idx = train_test_split(np.arange(len(PPMI_labels)), test_size=.4, random_state=rand_seed)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=.4, random_state=rand_seed)\n",
    "train_idx.sort()\n",
    "val_idx.sort()\n",
    "test_idx.sort()\n",
    "\n",
    "np.savez(val_train_test_idx_file,\n",
    "         val_idx=val_idx,\n",
    "         train_idx=train_idx,\n",
    "         test_idx=test_idx)\n",
    "\n",
    "train_val_test_stats(PPMI_labels, train_idx, val_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6a12a",
   "metadata": {},
   "source": [
    "<a id='define_metapaths'></a>\n",
    "## Defining metapaths and searching for metapath instances\n",
    "\n",
    "Note here the difference from biochemical pathways: metapath ≠ pathway\n",
    "\n",
    "The formal definition of a metapath $m$ for graph $G(V, E)$ with node types $V \\to T$ and edge types $E \\to R$ for relationship type is characterized as:\n",
    "    \n",
    "$$\n",
    "\\begin{align}\n",
    "    m = T_{1} \\stackrel{R_{1}}{\\longrightarrow} T_{2} \\stackrel{R_{2}}{\\longrightarrow} \\ldots \\stackrel{R_{l}}{\\longrightarrow} T_{l+1}\n",
    "\\end{align}\n",
    "$$\n",
    "    \n",
    "With $T=T_{1} \\circ T_{2} \\circ \\cdots \\circ T_{l}$ and $R=R_{1} \\circ R_{2} \\circ \\cdots \\circ R_{l}$ and $l$ the length of the metapath. \n",
    "\n",
    "This work uses 2 metapaths:\n",
    "- Metabolite – Protein – Metabolite (MPM)  \n",
    "- Metabolite – Protein – Protein – Metabolite (MPPM) \n",
    "\n",
    "The following figure should help explain the concept of a metapath and metapath instances:\n",
    "\n",
    "<div> <img src=\"images/metapath_instance_explainer.png\" width=\"800\"/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a75d7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metapath_neighbor_pairs_PPMI(M, type_mask, metapaths):\n",
    "    outputs = []\n",
    "    for metapath in metapaths:\n",
    "        print('Getting neighbor pairs for ', metapath)\n",
    "        # consider only the edges relevant to the expected metapath\n",
    "        mask = np.zeros(M.shape, dtype=bool)\n",
    "        for i in range(len(metapath)-1):\n",
    "            temp = np.zeros(M.shape, dtype=bool)\n",
    "            temp[np.ix_(type_mask == metapath[i], type_mask == metapath[i + 1])] = True\n",
    "            temp[np.ix_(type_mask == metapath[i + 1], type_mask == metapath[i])] = True\n",
    "            mask = np.logical_or(mask, temp)\n",
    "        partial_g_nx = nx.from_numpy_matrix((M * mask).astype(int))\n",
    "        \n",
    "        metapath_neighbor_paris = {}\n",
    "        for source in (type_mask == metapath[0]).nonzero()[0]:\n",
    "            if source == 0 or source % 5 == 0:            \n",
    "                print('  Node', source)\n",
    "            for target in (type_mask == metapath[-1]).nonzero()[0]:\n",
    "                has_path = False\n",
    "                single_source_paths = nx.single_source_shortest_path(\n",
    "                    partial_g_nx, source, cutoff=len(metapath))\n",
    "                if target in single_source_paths:\n",
    "                    has_path = True\n",
    "\n",
    "                if has_path:\n",
    "                    shortests = [p for p in nx.all_shortest_paths(partial_g_nx, source, target) if\n",
    "                                 len(p) == len(metapath)]\n",
    "                    if len(shortests) > 0:\n",
    "                        metapath_neighbor_paris[(source, target)] = shortests       \n",
    "        \n",
    "        outputs.append(metapath_neighbor_paris)\n",
    "    return outputs\n",
    "\n",
    "def find_metapaths_from_source(metapath, type_mask, partial_g_nx, source):\n",
    "    metapath_neighbor_pairs_source = {}\n",
    "#     if source == 0 or source % 5 == 0:            \n",
    "    print(f'  Node {source}', )\n",
    "    for target in (type_mask == metapath[-1]).nonzero()[0]:\n",
    "        has_path = False\n",
    "        single_source_paths = nx.single_source_shortest_path(\n",
    "            partial_g_nx, source, cutoff=len(metapath))\n",
    "        if target in single_source_paths:\n",
    "            has_path = True\n",
    "\n",
    "        if has_path:\n",
    "            shortests = [p for p in nx.all_shortest_paths(partial_g_nx, source, target) if\n",
    "                         len(p) == len(metapath)]\n",
    "            if len(shortests) > 0:\n",
    "                metapath_neighbor_pairs_source[(source, target)] = shortests       \n",
    "    return metapath_neighbor_pairs_source\n",
    "\n",
    "#To speed up the process the _MP version of the function calculates metapath instances parrallized\n",
    "def get_metapath_neighbor_pairs_PPMI_MP(M, type_mask, metapaths):\n",
    "    outputs = []\n",
    "    for metapath in metapaths:\n",
    "        print('Getting neighbor pairs for ', metapath)\n",
    "        # consider only the edges relevant to the expected metapath\n",
    "        mask = np.zeros(M.shape, dtype=bool)\n",
    "        for i in range(len(metapath)-1):\n",
    "            temp = np.zeros(M.shape, dtype=bool)\n",
    "            temp[np.ix_(type_mask == metapath[i], type_mask == metapath[i + 1])] = True\n",
    "            temp[np.ix_(type_mask == metapath[i + 1], type_mask == metapath[i])] = True\n",
    "            mask = np.logical_or(mask, temp)\n",
    "        partial_g_nx = nx.from_numpy_matrix((M * mask).astype(int))\n",
    "        \n",
    "        metapath_neighbor_paris = {}\n",
    "        a_pool = multiprocessing.Pool()\n",
    "        func = partial(find_metapaths_from_source, metapath, type_mask, partial_g_nx)\n",
    "        result = a_pool.map(func, (type_mask == metapath[0]).nonzero()[0])  \n",
    "        a_pool.close()\n",
    "        a_pool.join()\n",
    "        metapath_neighbor_paris = dict(ChainMap(*result))\n",
    "        \n",
    "        outputs.append(metapath_neighbor_paris)\n",
    "    return outputs\n",
    "\n",
    "def get_node_names(i_nodes):\n",
    "    node_names = list(PPMI_pruned.nodes)\n",
    "    return [node_names[i_node] for i_node in i_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00ac67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining MPM and MPPM metapaths\n",
    "MPM_metapath = (types['metabolite'], types['protein'], types['metabolite'])\n",
    "MPPM_metapath = (types['metabolite'], types['protein'], types['protein'], types['metabolite'])\n",
    "\n",
    "PPMI_expected_metapaths = [\n",
    "    [MPM_metapath, MPPM_metapath], #metabolite metapaths\n",
    "    [] #protein metapaths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf789d40",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting neighbor pairs for  (0, 1, 0)\n",
      "  Node 0\n",
      "  Node 8\n",
      "  Node 16\n",
      "  Node 24\n",
      "  Node 32\n",
      "  Node 40\n",
      "  Node 48  Node 56\n",
      "\n",
      "  Node 49\n",
      "  Node 25\n",
      "  Node 17\n",
      "  Node 1\n",
      "  Node 33\n",
      "  Node 34\n",
      "  Node 35\n",
      "  Node 9\n",
      "  Node 50\n",
      "  Node 18\n",
      "  Node 2\n",
      "  Node 41\n",
      "  Node 19\n",
      "  Node 57\n",
      "  Node 26\n",
      "  Node 3\n",
      "  Node 10\n",
      "  Node 42\n",
      "  Node 51\n",
      "  Node 36\n",
      "  Node 43\n",
      "  Node 11\n",
      "  Node 27\n",
      "  Node 52\n",
      "  Node 4\n",
      "  Node 44\n",
      "  Node 53\n",
      "  Node 58\n",
      "  Node 37\n",
      "  Node 28\n",
      "  Node 45\n",
      "  Node 29\n",
      "  Node 5\n",
      "  Node 20\n",
      "  Node 12\n",
      "  Node 30\n",
      "  Node 38\n",
      "  Node 59\n",
      "  Node 6\n",
      "  Node 46\n",
      "  Node 31\n",
      "  Node 47\n",
      "  Node 54\n",
      "  Node 13  Node 39\n",
      "\n",
      "  Node 64\n",
      "  Node 21\n",
      "  Node 72\n",
      "  Node 7\n",
      "  Node 60\n",
      "  Node 14\n",
      "  Node 80\n",
      "  Node 81\n",
      "  Node 82\n",
      "  Node 55\n",
      "  Node 15\n",
      "  Node 65\n",
      "  Node 61\n",
      "  Node 83\n",
      "  Node 73\n",
      "  Node 88\n",
      "  Node 74\n",
      "  Node 66\n",
      "  Node 84\n",
      "  Node 67  Node 62\n",
      "\n",
      "  Node 68\n",
      "  Node 89\n",
      "  Node 90\n",
      "  Node 69\n",
      "  Node 96\n",
      "  Node 70\n",
      "  Node 91\n",
      "  Node 92\n",
      "  Node 63\n",
      "  Node 85\n",
      "  Node 93\n",
      "  Node 22\n",
      "  Node 86\n",
      "  Node 71\n",
      "  Node 104\n",
      "  Node 97\n",
      "  Node 105\n",
      "  Node 94\n",
      "  Node 23\n",
      "  Node 87\n",
      "  Node 112\n",
      "  Node 98\n",
      "  Node 75\n",
      "  Node 120\n",
      "  Node 95\n",
      "  Node 121\n",
      "  Node 76\n",
      "  Node 99\n",
      "  Node 77\n",
      "  Node 100\n",
      "  Node 106\n",
      "  Node 113\n",
      "  Node 114\n",
      "  Node 78\n",
      "\n",
      "  Node 128  Node 122\n",
      "  Node 115\n",
      "  Node 129\n",
      "  Node 123\n",
      "  Node 130\n",
      "  Node 136\n",
      "  Node 116\n",
      "  Node 137\n",
      "  Node 131\n",
      "  Node 79\n",
      "  Node 132\n",
      "  Node 144\n",
      "  Node 117\n",
      "  Node 107\n",
      "  Node 124\n",
      "  Node 138\n",
      "  Node 125\n",
      "  Node 118\n",
      "  Node 152\n",
      "  Node 139\n",
      "  Node 126\n",
      "  Node 140\n",
      "  Node 133\n",
      "  Node 134\n",
      "  Node 119\n",
      "  Node 127\n",
      "  Node 108\n",
      "  Node 101\n",
      "  Node 145\n",
      "  Node 146\n",
      "  Node 135\n",
      "  Node 141\n",
      "  Node 153\n",
      "  Node 109\n",
      "  Node 154\n",
      "  Node 160\n",
      "  Node 155\n",
      "  Node 161\n",
      "  Node 168\n",
      "  Node 156\n",
      "  Node 142\n",
      "  Node 110\n",
      "  Node 157\n",
      "  Node 143\n",
      "  Node 147\n",
      "  Node 158\n",
      "  Node 102\n",
      "  Node 159\n",
      "  Node 176\n",
      "  Node 162\n",
      "  Node 169\n",
      "  Node 184\n",
      "  Node 192\n",
      "  Node 103\n",
      "  Node 148\n",
      "  Node 177\n",
      "  Node 200\n",
      "  Node 149\n",
      "  Node 163\n",
      "  Node 111\n",
      "  Node 170\n",
      "  Node 185\n",
      "  Node 164\n",
      "  Node 150\n",
      "  Node 208\n",
      "  Node 151\n",
      "  Node 186\n",
      "  Node 171\n",
      "  Node 178\n",
      "  Node 201\n",
      "  Node 209\n",
      "  Node 193\n",
      "  Node 202\n",
      "  Node 216\n",
      "  Node 187\n",
      "  Node 203\n",
      "  Node 165\n",
      "\n",
      "  Node 188  Node 172\n",
      "  Node 204\n",
      "  Node 173\n",
      "  Node 174\n",
      "  Node 179\n",
      "  Node 217\n",
      "  Node 189\n",
      "  Node 218\n",
      "  Node 210\n",
      "  Node 219\n",
      "  Node 194\n",
      "  Node 166\n",
      "  Node 220\n",
      "  Node 195\n",
      "  Node 180\n",
      "  Node 205\n",
      "  Node 181\n",
      "  Node 182\n",
      "  Node 183\n",
      "  Node 175\n",
      "  Node 190\n",
      "  Node 221\n",
      "  Node 211\n",
      "  Node 212\n",
      "  Node 222\n",
      "  Node 196\n",
      "  Node 206\n",
      "  Node 197\n",
      "  Node 223  Node 207\n",
      "\n",
      "  Node 167\n",
      "  Node 191\n",
      "  Node 198\n",
      "  Node 224\n",
      "  Node 225\n",
      "  Node 226\n",
      "  Node 199\n",
      "  Node 227\n",
      "  Node 213\n",
      "  Node 214\n",
      "  Node 215\n",
      "Getting neighbor pairs for  (0, 1, 1, 0)\n",
      "  Node 0\n",
      "\n",
      "  Node 8  Node 16\n",
      "  Node 24\n",
      "  Node 32\n",
      "  Node 40\n",
      "  Node 48\n",
      "  Node 56\n",
      "  Node 49\n",
      "  Node 25\n",
      "  Node 9\n",
      "  Node 57\n",
      "  Node 17\n",
      "  Node 1\n",
      "  Node 33\n",
      "  Node 41\n",
      "  Node 50\n",
      "  Node 10\n",
      "  Node 26\n",
      "  Node 58\n",
      "  Node 51\n",
      "  Node 34\n",
      "  Node 18\n",
      "  Node 11\n",
      "  Node 42\n",
      "  Node 2\n",
      "  Node 27\n",
      "  Node 59\n",
      "  Node 52\n",
      "  Node 19\n",
      "  Node 28\n",
      "  Node 60\n",
      "  Node 12\n",
      "  Node 35\n",
      "  Node 53\n",
      "  Node 43\n",
      "  Node 3\n",
      "  Node 20\n",
      "  Node 29\n",
      "  Node 61\n",
      "  Node 54\n",
      "  Node 13\n",
      "  Node 44\n",
      "  Node 36\n",
      "  Node 4\n",
      "  Node 21\n",
      "  Node 62\n",
      "  Node 30\n",
      "  Node 55\n",
      "  Node 37\n",
      "  Node 31\n",
      "  Node 63\n",
      "  Node 45  Node 14\n",
      "\n",
      "  Node 64\n",
      "  Node 5\n",
      "  Node 22\n",
      "  Node 38\n",
      "  Node 72\n",
      "  Node 80\n",
      "  Node 15\n",
      "  Node 39\n",
      "  Node 46\n",
      "  Node 65\n",
      "  Node 81\n",
      "  Node 6\n",
      "  Node 73\n",
      "  Node 23\n",
      "  Node 88\n",
      "  Node 96\n",
      "  Node 82\n",
      "  Node 74\n",
      "  Node 47\n",
      "  Node 89\n",
      "  Node 66\n",
      "  Node 7\n",
      "  Node 104\n",
      "  Node 97\n",
      "  Node 90\n",
      "  Node 83\n",
      "  Node 75\n",
      "  Node 105\n",
      "  Node 112\n",
      "  Node 98\n",
      "  Node 91\n",
      "  Node 67\n",
      "  Node 84\n",
      "  Node 120\n",
      "  Node 106\n",
      "  Node 99\n",
      "  Node 76\n",
      "  Node 85\n",
      "  Node 92\n",
      "  Node 121\n",
      "  Node 113\n",
      "  Node 68\n",
      "  Node 107\n",
      "  Node 100\n",
      "  Node 122\n",
      "  Node 114\n",
      "  Node 86\n",
      "  Node 77\n",
      "  Node 93\n",
      "  Node 108\n",
      "  Node 101\n",
      "  Node 69\n",
      "  Node 115\n",
      "  Node 78\n",
      "  Node 123\n",
      "  Node 70\n",
      "  Node 109\n",
      "  Node 102\n",
      "  Node 87\n",
      "  Node 94\n",
      "  Node 79\n",
      "  Node 103\n",
      "  Node 110\n",
      "  Node 116\n",
      "  Node 95\n",
      "  Node 124  Node 71\n",
      "\n",
      "  Node 128\n",
      "  Node 136\n",
      "  Node 144\n",
      "  Node 111\n",
      "  Node 125\n",
      "  Node 152\n",
      "  Node 117\n",
      "  Node 145\n",
      "  Node 160\n",
      "  Node 129\n",
      "  Node 137\n",
      "  Node 126\n",
      "  Node 168\n",
      "  Node 118\n",
      "  Node 153\n",
      "  Node 146\n",
      "  Node 161\n",
      "  Node 127\n",
      "  Node 130\n",
      "  Node 138\n",
      "  Node 119\n",
      "  Node 169\n",
      "  Node 147\n",
      "  Node 162\n",
      "  Node 154\n",
      "  Node 176\n",
      "  Node 184\n",
      "  Node 163\n",
      "  Node 139\n",
      "  Node 177\n",
      "  Node 131\n",
      "  Node 155\n",
      "  Node 148\n",
      "  Node 170\n",
      "  Node 185\n",
      "  Node 140\n",
      "  Node 156\n",
      "  Node 178\n",
      "  Node 164\n",
      "  Node 186\n",
      "  Node 132\n",
      "  Node 149\n",
      "  Node 171\n",
      "  Node 179\n",
      "  Node 157\n",
      "  Node 141\n",
      "  Node 187\n",
      "  Node 158\n",
      "  Node 180\n",
      "  Node 133\n",
      "  Node 165\n",
      "  Node 172\n",
      "  Node 142\n",
      "  Node 150\n",
      "  Node 188\n",
      "  Node 181\n",
      "  Node 159\n",
      "  Node 143\n",
      "  Node 182\n",
      "  Node 189\n",
      "  Node 173\n",
      "  Node 134\n",
      "  Node 166\n",
      "  Node 151\n",
      "  Node 192\n",
      "  Node 183\n",
      "  Node 190\n",
      "  Node 135\n",
      "  Node 200\n",
      "  Node 167\n",
      "  Node 174\n",
      "  Node 193\n",
      "  Node 208\n",
      "  Node 216\n",
      "  Node 191\n",
      "  Node 224\n",
      "  Node 225\n",
      "  Node 194\n",
      "  Node 217\n",
      "  Node 201\n",
      "  Node 175\n",
      "  Node 209\n",
      "  Node 226\n",
      "  Node 195\n",
      "  Node 218\n",
      "  Node 202\n",
      "  Node 227\n",
      "  Node 210\n",
      "  Node 219\n",
      "  Node 196\n",
      "  Node 203\n",
      "  Node 197\n",
      "  Node 211\n",
      "  Node 220\n",
      "  Node 204\n",
      "  Node 198\n",
      "  Node 212\n",
      "  Node 221\n",
      "  Node 205\n",
      "  Node 199\n",
      "  Node 213\n",
      "  Node 222\n",
      "  Node 214\n",
      "  Node 206\n",
      "  Node 215\n",
      "  Node 223\n",
      "  Node 207\n",
      "(31940, 3)\n",
      "(24450, 4)\n"
     ]
    }
   ],
   "source": [
    "#Obtaining metapaths can take quite a bit of time, order of tens of minutes to hours, depending on the hardware specs\n",
    "#The original MAGNN metapath instance calculater did not allow for the MPPM metapath, so it was adjusted here\n",
    "type_i = 'metabolite'\n",
    "PPMI_neighbor_pairs = get_metapath_neighbor_pairs_PPMI_MP(PPMI_adjM, PPMI_type_mask, PPMI_expected_metapaths[types[type_i]])\n",
    "du.dump_in_pickle(metapaths_metabolites_file, PPMI_neighbor_pairs)\n",
    "G_list = utils.preprocess.get_networkx_graph(PPMI_neighbor_pairs, PPMI_type_mask, types[type_i])\n",
    "for G, metapath in zip(G_list, PPMI_expected_metapaths[types[type_i]]):\n",
    "    filename = 'metapath_graph_' + '-'.join(map(str, metapath)) + '.adjlist'\n",
    "    metapaths_metabolites_file = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', filename)\n",
    "    nx.write_adjlist(G, metapaths_metabolites_file)\n",
    "all_edge_metapath_idx_array = utils.preprocess.get_edge_metapath_idx_array(PPMI_neighbor_pairs)\n",
    "for metapath, edge_metapath_idx_array in zip(PPMI_expected_metapaths[types[type_i]], all_edge_metapath_idx_array):\n",
    "    filename = 'edges_' + '-'.join(map(str, metapath)) + '_idx.npy'\n",
    "    edges_file = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', filename)\n",
    "    np.save(edges_file, edge_metapath_idx_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00fc8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code below should be run for storing metapath instance information in MAGNN readable format\n",
    "#This allows for the batch training\n",
    "type_i = 'metabolite'\n",
    "target_idx_list = np.arange(len(PPMI_labels))\n",
    "for metapath in PPMI_expected_metapaths[types[type_i]]:\n",
    "    filename = 'edges_' + '-'.join(map(str, metapath)) + '_idx.npy'\n",
    "    edges_file = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', filename)\n",
    "    edge_metapath_idx_array = np.load(edges_file)\n",
    "    target_metapaths_mapping = {}\n",
    "    for target_idx in target_idx_list:\n",
    "        target_metapaths_mapping[target_idx] = edge_metapath_idx_array[edge_metapath_idx_array[:, 0] == target_idx][:, ::-1]\n",
    "    filename_new = 'edges_' + '-'.join(map(str, metapath)) + '_idx.pickle'\n",
    "    edges_file_new = du.get_file_path(data_dir, 'MAGNN', 'preprocessed', filename_new)\n",
    "    du.dump_in_pickle(edges_file_new, target_metapaths_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
